# Welcome to Challenge 2


## <u> Aim </u>


In this Challenge, we will:

- Create a training script using the code from Challenge 1
- Perform unit tests on the training script
- Run an experiment from a Python Script using the ScriptRunConfig() object
- Register the model into Azure ML and log the relevant metrics

!['Challenge 2 Architecture'](/Challenge23/images/challenge2architecture.png)

## <u> Steps </u>

As a team, complete the following tasks:

1. Upload your training data CSV into your **Challenge2/data** folder.

1. Refactor the code from the Challenge 1 notebook into a Python Script by completing the commented `##TODO` sections of the `train.py` cell in the Challenge 2 notebook.

1. Ensure the provided unit tests pass for `train.py` by calling [pytest](https://docs.pytest.org/en/6.2.x/) in the Terminal, which is started from the Notebook's UI. Browse the contents of the file with tests to understand what types of tests may be relevant.

    - Install `pytest` from `pip` if necessary - `pip install pytest`
    - `pytest test_train.py`


1. Make the `train.py` and `test_train.py` files pass linting with `flake8` in the Terminal. You do not need to fix every single linting error here. This is for your understanding of how to produce quality code and does not impact the output and outcome. Once you have a good understnading of what the errors mean and how you would resolve them, move on to the next step.

1. Complete and run the remaining cells of the Challenge 2 notebook. This notebook runs the training script as an experiment and enables you to review the outputs generated by the experiment run in the Azure ML studio. The outputs should include the experiment run log files and the trained model.

    - In `driver_training.py`, complete the `##TODO` for using the functions imported from `train.py`
    - In the ScriptRunConfig() object, create a compute cluster, environment and experiment using the links provided.

1. View the learning_rate parameter in the configuration file, execute a run against Azure ML compute to train the model and verify that the metrics are logged. Change the learning_rate value and then execute another run and see the values changed in the Azure ML service.

1. Register the model in the Azure ML model repository using the provided notebook code in your Azure ML workspace - using tags to record the AUC metric in the registration so that the quality of the model is registered and not just the run.


## <u> Success Criteria </u>  

To have completed this Challenge, you should have:  

- Have a fully functional Challenge 2 notebook (.ipynb)

    - Successfully runs your experiment on Azure ML and can see the logged AUC metrics and trained model in the run results using two different parameter configuration values.

    - Successfully registers the trained model and tags the model with the AUC metric.

- Demonstrate that the unit tests passs against the Python training code using `pytest`.

- Use `flake8` to demonstrate that `train.py` and `test_train.py` conform to the PEP 8 style guide for Python code.

- Do a local run of the notebook and show the metrics associated both with the run and the registered model in Azure ML.


## <u> Coaching Questions </u>

Once you've completed the Challenge, tag your Coach in your Team Channel and discuss the following questions with them:

1. What is the benefit of separating the training code out of the notebook?

1. What is the benefit of running your experiments using Azure Machine Learning?

1. What are the benefits of using a model repository and tracking the metrics and other parameters?

1. What is your experience in your organization/customers regarding doing things like model registration and tracking training metrics?