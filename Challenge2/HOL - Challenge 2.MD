# Welcome to Challenge 2


## <u> Aim </u>


In this Challenge, we will:

- Create a training script using the code from Challenge 1
- Perform unit tests on the training script
- Run an experiment from a Python Script using the ScriptRunConfig() object
- Register the model into Azure ML and log the relevant metrics

!['Challenge 2 Architecture'](/Challenge2/images/challenge2architecture.PNG)

## <u> Steps </u>

As a team, complete the following tasks:

1. Refactor the code from the Challenge 1 notebook into a Python Script by completing the commented `##TODO` sections of the `train.py` cell in the Challenge 2 notebook.

1. Ensure the provided unit tests pass by calling `pytest` in the Terminal, which is started from the Notebook's UI. Browse the contents of the file with tests to understand what types of tests may be relevant.

    - Install `pytest` from `pip` if necessary.

1. Make the `train.py` and `test_train.py` files pass linting with `flake8` in the Terminal.

1. Complete and run the remaining sections of the Challenge 2 notebook. This notebook runs the training script as an experiment and enables you to review the outputs generated by the experiment run in the Azure ML studio. The outputs should include the experiment run log files and the trained model.

    - Specify a local compute target for running you experiment which will run against the existing Compute Instance. In Production, you would likely use a remote training cluster.
    - Log the AUC evaluation metric in the experiment run, as well as each of the parameter values used to train the model.

1. View the learning_rate parameter in the configuration file, execute a run against Azure ML compute to train the model and verify that the metrics are logged. Change the learning_rate value and then execute another run and see the values changed in the Azure ML service.

1. Register the model in the Azure ML model repository using the provided notebook code in your Azure ML workspace - using tags to record the AUC metric in the registration so that the quality of the model is registered and not just the run.


## <u> Success Criteria </u>  

To have completed this Challenge, you should have:  

- Have a fully functional Challenge 2 notebook (.ipynb)

    - Successfully runs your experiment on Azure ML and can see the logged AUC metrics and trained model in the run results using two different parameter configuration values.

    - Successfully registers the trained model and tags the model with the AUC metric.

- Demonstrate that the unit tests passs against the Python training code using `pytest`.

- Use `flake8` to demonstrate that `train.py` and `test_train.py` conform to the PEP 8 style guide for Python code.

- Do a local run of the notebook and show the metrics associated both with the run and the registered model in Azure ML.


## <u> Coaching Questions </u>

Once you've completed the Challenge, tag your Coach in your Team Channel and discuss the following questions with them:

1. What is the benefit of separating the training code out of the notebook?

1. What is the benefit of running your experiments using Azure Machine Learning?

1. What are the benefits of using a model repository and tracking the metrics and other parameters?

1. What is your experience in your organization/customers regarding doing things like model registration and tracking training metrics?